import streamlit as st
import litstudy
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import numpy as np
import seaborn as sns
import pandas as pd
import streamlit.components.v1 as components
from wordcloud import WordCloud
import os

# --- CONFIGURARE PAGINÄ‚ ---
st.set_page_config(
    page_title="LitStudy Pro - AnalizÄƒ BibliometricÄƒ",
    layout="wide",
    page_icon="ğŸ“š"
)

# --- CSS CUSTOM ---
st.markdown("""
<style>
    .main { background-color: #f9f9f9; }
    h1 { color: #2c3e50; }
    .stButton>button { width: 100%; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“š LitStudy Pro: AnalizÄƒ BibliometricÄƒ AvansatÄƒ")
st.markdown("### Instrument pentru analiza automatÄƒ a literaturii È™tiinÈ›ifice")

# --- 1. SIDEBAR: DATA LOADING ---
st.sidebar.header("1. SursÄƒ Date")
sursa_date = st.sidebar.radio("Metoda de import:", ("CÄƒutare Live (DBLP)", "FiÈ™ier Local"))

# IniÈ›ializÄƒm session_state pentru a nu pierde datele la refresh (filtrare)
if 'docs' not in st.session_state:
    st.session_state['docs'] = []

# LOGICA DE ÃNCÄ‚RCARE
new_docs = []

if 'regenerate_word_cloud' not in st.session_state:
    st.session_state['regenerate_word_cloud'] = False
regenerate_word_cloud = st.session_state['regenerate_word_cloud'] 

if sursa_date == "CÄƒutare Live (DBLP)":
    query = st.sidebar.text_input("Cuvinte cheie", value="Machine Learning")
    limit_docs = st.sidebar.slider("Nr. maxim articole", 100, 500, 100, step=100)
    
    if st.sidebar.button("ğŸ” CautÄƒ pe DBLP"):
        with st.spinner('Se descarcÄƒ datele de pe DBLP...'):
            try:
                new_docs = litstudy.search_dblp(query, limit=limit_docs)
                regenerate_word_cloud = True
                st.session_state['docs'] = new_docs
                st.success(f"GÄƒsite: {len(new_docs)} articole.")
            except Exception as e:
                st.error(f"Eroare: {e}")

else:
    uploaded_file = st.sidebar.file_uploader("ÃncarcÄƒ fiÈ™ier (BIB, RIS, CSV)", type=["bib", "ris", "csv"])
    if uploaded_file:
        temp_name = f"temp_{uploaded_file.name}"
        with open(temp_name, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        try:
            with st.spinner('Se proceseazÄƒ fiÈ™ierul...'):
                if temp_name.endswith(".bib"):
                    new_docs = litstudy.load_bibtex(temp_name)
                elif temp_name.endswith(".ris"):
                    new_docs = litstudy.load_ris(temp_name)
                elif temp_name.endswith(".csv"):
                    new_docs = litstudy.load_csv(temp_name)
                st.session_state['docs'] = new_docs
                st.sidebar.success(f"FiÈ™ier Ã®ncÄƒrcat: {uploaded_file.name}")
        except Exception as e:
            st.sidebar.error(f"Eroare fiÈ™ier: {e}")

# PreluÄƒm documentele din memorie
docs = st.session_state['docs']

# --- 2. SISTEM DE FILTRARE (CERINÈšA 5) ---
filtered_docs = docs
if docs:
    st.sidebar.markdown("---")
    st.sidebar.header("2. Filtrare Rezultate")
    
    # A. Filtru Ani
    years = [d.publication_year for d in docs if d.publication_year is not None]
    if years:
        min_y, max_y = int(min(years)), int(max(years))
        def slider_change_callback():
            st.session_state['regenerate_word_cloud'] = True
        sel_years = st.sidebar.slider("ğŸ“… Interval Ani", min_y, max_y, (min_y, max_y), key='my_slider', on_change=slider_change_callback)
        filtered_docs = [d for d in filtered_docs if d.publication_year and sel_years[0] <= d.publication_year <= sel_years[1]]

    # B. Filtru SursÄƒ (Jurnal)
    sources = list(set([d.source for d in docs if hasattr(d, 'source') and d.source]))
    if sources:
        sel_source = st.sidebar.multiselect("ğŸ“– Filtru Jurnal/ConferinÈ›Äƒ", sources)
        if sel_source:
            filtered_docs = [d for d in filtered_docs if hasattr(d, 'source') and d.source in sel_source]

    st.sidebar.info(f"Se analizeazÄƒ: **{len(filtered_docs)}** / {len(docs)} articole")

# --- 3. INTERFAÈšA PRINCIPALÄ‚ ---
if filtered_docs:
    # Definim 4 Tab-uri pentru a acoperi toate cerinÈ›ele
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“Š Dashboard Statistici", 
        "ğŸ§  Topic Modeling (NLP)", 
        "ğŸ•¸ï¸ ReÈ›ele", 
        "ğŸ“¥ Export Date"
    ])

    # === TAB 1: STATISTICI EXTINSE (CERINÈšA 2) ===
    with tab1:
        st.subheader("Privire de ansamblu")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**PublicaÈ›ii pe ani**")
            fig1 = plt.figure(figsize=(8, 4))
            litstudy.plot_year_histogram(filtered_docs)
            st.pyplot(fig1, use_container_width=True)
            
        with col2:
            st.markdown("**Top Autori**")
            fig2 = plt.figure(figsize=(8, 4))
            litstudy.plot_author_histogram(filtered_docs, limit=10)
            st.pyplot(fig2, use_container_width=True)

        st.markdown("---")
        
        # Statistici Extra: Surse
        col3, col4 = st.columns(2)
        with col3:
            st.markdown("**Top LocaÈ›ii de Publicare (Venues)**")
            try:
                fig3 = plt.figure(figsize=(8, 4))
                litstudy.plot_source_histogram(filtered_docs, limit=10)
                plt.xticks(rotation=45, ha='right')
                st.pyplot(fig3, use_container_width=True)
            except:
                st.warning("Nu existÄƒ date despre surse.")

        with col4:
            st.markdown("**Word Cloud (Din Titluri)**")
            # Generare rapidÄƒ WordCloud
            text = " ".join([d.title for d in filtered_docs if d.title])
            if text:
                if 'wc' not in st.session_state:
                    st.session_state['wc'] = None
                wc = st.session_state['wc']
                if regenerate_word_cloud or wc is None:
                    wc = WordCloud(width=800, height=400, background_color='white').generate(text)
                    st.session_state['wc'] = wc
                    st.session_state['regenerate_word_cloud'] = False
                fig_wc = plt.figure(figsize=(8, 4))
                plt.imshow(wc, interpolation='bilinear')
                plt.axis("off")
                st.pyplot(fig_wc, use_container_width=True)

    # === TAB 2: TOPIC MODELING (Implementare "Low-Level" Scikit-Learn) ===
    with tab2:
        st.subheader("Detectare AutomatÄƒ a Subiectelor (NMF)")
        st.markdown("""
        > AceastÄƒ secÈ›iune implementeazÄƒ algoritmul **NMF (Non-negative Matrix Factorization)** folosind direct 
        > biblioteca *Scikit-Learn* pentru o precizie maximÄƒ È™i control total asupra datelor.
        """)

        if len(filtered_docs) < 10:
            st.warning("âš ï¸ Ai nevoie de cel puÈ›in 10 articole pentru a genera topicuri relevante.")
        else:
            col_settings, col_viz = st.columns([1, 3])
            
            with col_settings:
                st.markdown("**SetÄƒri Model**")
                num_topics = st.slider("NumÄƒr de topicuri", 3, 10, 5)
                run_nlp = st.button("ğŸš€ RuleazÄƒ Analiza")

            with col_viz:
                if run_nlp:
                    with st.spinner("Se proceseazÄƒ textul È™i se antreneazÄƒ modelul NMF..."):
                        try:
                            # 1. PREGÄ‚TIRE DATE (Extragem textul din obiectele LitStudy)
                            # CombinÄƒm titlul cu abstractul (dacÄƒ existÄƒ) pentru fiecare articol
                            text_data = []
                            for doc in filtered_docs:
                                content = doc.title
                                if hasattr(doc, 'abstract') and doc.abstract:
                                    content += " " + doc.abstract
                                text_data.append(content)

                            # 2. VECTORIZARE (TF-IDF)
                            # TransformÄƒm textul Ã®n numere, eliminÃ¢nd cuvintele comune (stop words)
                            tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
                            tfidf = tfidf_vectorizer.fit_transform(text_data)
                            feature_names = tfidf_vectorizer.get_feature_names_out()

                            # 3. ANTRENARE MODEL NMF
                            nmf_model = NMF(n_components=num_topics, random_state=42, init='nndsvd')
                            nmf_model.fit(tfidf)
                            
                            st.success("AnalizÄƒ finalizatÄƒ cu succes (Scikit-Learn Backend)!")
                            st.markdown("### ğŸ§© Rezultate Identificate:")

                            # 4. EXTRAGERE È˜I AFIÈ˜ARE TOPICURI
                            for topic_idx, topic in enumerate(nmf_model.components_):
                                # LuÄƒm top 10 cuvinte cu cea mai mare greutate Ã®n topic
                                top_indices = topic.argsort()[:-11:-1]
                                top_words = [feature_names[i] for i in top_indices]
                                
                                with st.expander(f"Topic {topic_idx + 1}: {top_words[0].upper()}..."):
                                    st.write(f"**Cuvinte cheie:** {', '.join(top_words)}")
                                    # AfiÈ™Äƒm un mini grafic de bare pentru importanÈ›a cuvintelor (Bonus vizual)
                                    topic_df = pd.DataFrame({
                                        'CuvÃ¢nt': top_words,
                                        'ImportanÈ›Äƒ': topic[top_indices]
                                    })
                                    st.bar_chart(topic_df.set_index('CuvÃ¢nt'))

                        except Exception as e:
                            st.error(f"A apÄƒrut o eroare la procesare: {e}")
                            st.info("VerificÄƒ dacÄƒ articolele selectate au abstracte disponibile.")

    # === TAB 3: REÈšELE ===
    with tab3:
        st.subheader("ReÈ›ea de Co-autori")
        st.info("AceastÄƒ vizualizare aratÄƒ grupurile de cercetÄƒtori care colaboreazÄƒ frecvent.")
        
        try:
            net = litstudy.build_coauthor_network(filtered_docs)
            if net and len(net.nodes) > 0:
                html_file = "network.html"
                litstudy.plot_network(net, height="600px")
                
                # Hack pentru a citi fiÈ™ierul generat de litstudy
                # De obicei Ã®l salveazÄƒ ca 'citation.html' sau deschide temp
                if os.path.exists("citation.html"):
                    with open("citation.html", 'r', encoding='utf-8') as f:
                        html_src = f.read()
                    components.html(html_src, height=620, scrolling=True)
                else:
                    st.warning("ReÈ›eaua a fost generatÄƒ Ã®n fundal.")
            else:
                st.warning("Nu existÄƒ suficiente conexiuni pentru o reÈ›ea.")
        except Exception as e:
            st.error(f"Eroare reÈ›ea: {e}")

    # === TAB 4: DATE & EXPORT (CERINÈšA 3) ===
    with tab4:
        st.subheader("Export Date")
        
        # Conversie la Pandas DataFrame
        data = []
        for d in filtered_docs:
            data.append({
                "Titlu": d.title,
                "An": d.publication_year,
                "Autori": ", ".join([a.name for a in d.authors]) if d.authors else "",
                "SursÄƒ": d.source if hasattr(d, 'source') else ""
            })
        
        df = pd.DataFrame(data)
        st.dataframe(df, use_container_width=True)
        
        st.markdown("### ğŸ“¥ DescarcÄƒ Raport")
        col_dl1, col_dl2 = st.columns(2)
        
        with col_dl1:
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "ğŸ“„ DescarcÄƒ Tabel (CSV)",
                data=csv,
                file_name="litstudy_export.csv",
                mime="text/csv"
            )
        
        with col_dl2:
            st.info("ğŸ’¡ Pentru raportul PDF complet, faceÈ›i o capturÄƒ de ecran a Tab-ului 'Dashboard Statistici' È™i includeÈ›i-o Ã®n documentaÈ›ie.")

elif not docs:
    st.info("ğŸ‘ˆ Ãncepe prin a cÄƒuta un termen sau a Ã®ncÄƒrca un fiÈ™ier Ã®n meniul din stÃ¢nga.")